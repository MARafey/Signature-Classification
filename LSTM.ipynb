{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-27 07:40:27.431854: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-27 07:40:27.506388: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-27 07:40:27.537335: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-27 07:40:27.546747: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-27 07:40:27.610292: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-27 07:40:28.460268: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data from text file \n",
    "text = open('/teamspace/studios/this_studio/Shakspear/alllines.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(text, sequence_length=10):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts([text])\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "    input_sequences = []\n",
    "    for line in text.split('\\n'):\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i + 1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "\n",
    "    # Pad sequences\n",
    "    input_sequences = pad_sequences(input_sequences, maxlen=sequence_length, padding='pre')\n",
    "    X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "    y = keras.utils.to_categorical(y, num_classes=total_words)\n",
    "\n",
    "    return X, y, total_words, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Build and Train the LSTM Model\n",
    "def build_model(sequence_length, total_words):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Embedding(total_words, 100, input_length=sequence_length - 1))\n",
    "    model.add(layers.LSTM(150))\n",
    "    model.add(layers.Dense(total_words, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y, model):\n",
    "    model.fit(X, y, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_word(model, tokenizer, input_text, sequence_length):\n",
    "    input_seq = tokenizer.texts_to_sequences([input_text])[0]\n",
    "    input_seq = pad_sequences([input_seq], maxlen=sequence_length - 1, padding='pre')\n",
    "    predicted = model.predict(input_seq, verbose=0)\n",
    "    return tokenizer.index_word[np.argmax(predicted)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 135. GiB for an array with shape (709820, 25576) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X, y, total_words, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m build_model(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], total_words)\n\u001b[1;32m      4\u001b[0m train_model(X, y, model)\n",
      "Cell \u001b[0;32mIn[22], line 16\u001b[0m, in \u001b[0;36mprepare_sequences\u001b[0;34m(text, sequence_length)\u001b[0m\n\u001b[1;32m     14\u001b[0m input_sequences \u001b[38;5;241m=\u001b[39m pad_sequences(input_sequences, maxlen\u001b[38;5;241m=\u001b[39msequence_length, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpre\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m X, y \u001b[38;5;241m=\u001b[39m input_sequences[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], input_sequences[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 16\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_categorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y, total_words, tokenizer\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/utils/numerical_utils.py:97\u001b[0m, in \u001b[0;36mto_categorical\u001b[0;34m(x, num_classes)\u001b[0m\n\u001b[1;32m     95\u001b[0m     num_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     96\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 97\u001b[0m categorical \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m categorical[np\u001b[38;5;241m.\u001b[39marange(batch_size), x] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     99\u001b[0m output_shape \u001b[38;5;241m=\u001b[39m input_shape \u001b[38;5;241m+\u001b[39m (num_classes,)\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 135. GiB for an array with shape (709820, 25576) and data type float64"
     ]
    }
   ],
   "source": [
    "X, y, total_words, tokenizer = prepare_sequences(text)\n",
    "\n",
    "model = build_model(X.shape[1], total_words)\n",
    "train_model(X, y, model)\n",
    "\n",
    "# while True:\n",
    "#     input_text = input(\"Enter a partial sentence (or 'exit' to quit): \")\n",
    "#     if input_text.lower() == 'exit':\n",
    "#         break\n",
    "#     next_word = generate_next_word(model, tokenizer, input_text, X.shape[1])\n",
    "#     print(f\"Suggested next word: {next_word}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
